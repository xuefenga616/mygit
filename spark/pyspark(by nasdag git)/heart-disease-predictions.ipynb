{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Prediction System\n",
    "\n",
    "### Part 1: Data Crunching\n",
    "\n",
    "Patient's diagnosis data is available from several [hospitals](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed.cleveland.data  processed.switzerland.data\r\n",
      "processed.hungarian.data  processed.va.data\r\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data\n",
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data\n",
    "! ls processed.*.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14 attributes are sent for each patient. The \"goal\" field refers to the presence of heart disease in the patient - from 0 (no presence) to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0,1.0,1.0,145.0,233.0,1.0,2.0,150.0,0.0,2.3,3.0,0.0,6.0,0\r\n",
      "67.0,1.0,4.0,160.0,286.0,0.0,2.0,108.0,1.0,1.5,2.0,3.0,3.0,2\r\n",
      "67.0,1.0,4.0,120.0,229.0,0.0,2.0,129.0,1.0,2.6,2.0,2.0,7.0,1\r\n",
      "37.0,1.0,3.0,130.0,250.0,0.0,0.0,187.0,0.0,3.5,3.0,0.0,3.0,0\r\n",
      "41.0,0.0,2.0,130.0,204.0,0.0,2.0,172.0,0.0,1.4,1.0,0.0,3.0,0\r\n"
     ]
    }
   ],
   "source": [
    "! head -5 processed.cleveland.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a description of the attributes:\n",
    "\n",
    "    -- 1. age           age in years\n",
    "    -- 2. sex           sex (1 = male; 0 = female)\n",
    "    -- 3. cp            chest pain type\n",
    "                          - Value 1: typical angina\n",
    "                          - Value 2: atypical angina\n",
    "                          - Value 3: non-anginal pain\n",
    "                          - Value 4: asymptomatic\n",
    "    -- 4. trestbps      resting blood pressure (in mm Hg on admission to the hospital)\n",
    "    -- 5. chol          serum cholestoral in mg/dl\n",
    "    -- 6. fbs           fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "    -- 7. restecg       resting electrocardiographic results\n",
    "                          - Value 0: normal\n",
    "                          - Value 1: having ST-T wave abnormality (T wave inversions and/\n",
    "                                     or ST elevation or depression of > 0.05 mV)\n",
    "                          - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "    -- 8. thalach       maximum heart rate achieved\n",
    "    -- 9. exang         exercise induced angina (1 = yes; 0 = no)\n",
    "    -- 10. oldpeak      ST depression induced by exercise relative to rest\n",
    "    -- 11. slope        the slope of the peak exercise ST segment\n",
    "                          - Value 1: upsloping\n",
    "                          - Value 2: flat\n",
    "                          - Value 3: downsloping\n",
    "    -- 12. ca           number of major vessels (0-3) colored by flourosopy\n",
    "    -- 13. thal         3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "    -- 14. num          diagnosis of heart disease (angiographic disease status)\n",
    "                          - Value 0: < 50% diameter narrowing\n",
    "                          - Value 1: > 50% diameter narrowing (in any major vessel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous tutorial, we will hold for the time being the schema in memory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaStringADD = 'age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal,diagnosis'\n",
    "typeStringADD = 'INT,STRING,STRING,INT,INT,BOOLEAN,STRING,INT,BOOLEAN,FLOAT,STRING,INT,STRING,INT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and move the incoming data to the \"in\" directory of HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   1 nasdag supergroup       5523 2015-12-21 09:12 /user/datacruncher/in/orders20140102.csv\r\n",
      "-rw-r--r--   1 nasdag supergroup      12505 2015-12-21 09:12 /user/datacruncher/in/orders20140103.csv\r\n",
      "-rw-r--r--   1 nasdag supergroup      18461 2016-01-01 09:42 /user/datacruncher/in/processed.cleveland.data\r\n",
      "-rw-r--r--   1 nasdag supergroup      10263 2016-01-01 09:42 /user/datacruncher/in/processed.hungarian.data\r\n",
      "-rw-r--r--   1 nasdag supergroup       4109 2016-01-01 09:42 /user/datacruncher/in/processed.switzerland.data\r\n",
      "-rw-r--r--   1 nasdag supergroup       6737 2016-01-01 09:42 /user/datacruncher/in/processed.va.data\r\n"
     ]
    }
   ],
   "source": [
    "#! hdfs dfs -rm -r /user/datacruncher/in/processed.*.data\n",
    "#! hdfs dfs -rm -r /user/datacruncher/raw/processed.*.data\n",
    "#! hdfs dfs -rm -r /user/datacruncher/out/heart_disease*\n",
    "! hdfs dfs -put processed.*.data /user/datacruncher/in/\n",
    "! rm processed.*.data\n",
    "! hdfs dfs -ls /user/datacruncher/in/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's make sure that the JDBC access to the diagnosis data is available although it will not be used later in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age INT, sex STRING, cp STRING, trestbps INT, chol INT, fbs BOOLEAN, restecg STRING, thalach INT, exang BOOLEAN, oldpeak FLOAT, slope STRING, ca INT, thal STRING, diagnosis INT\n"
     ]
    }
   ],
   "source": [
    "import pyhs2\n",
    "\n",
    "sqlStringADD = ', '.join([' '.join(x) for x in zip(schemaStringADD.split(','), typeStringADD.split(','))])\n",
    "print sqlStringADD\n",
    "\n",
    "conn = pyhs2.connect(host='localhost', port=10000, authMechanism=\"PLAIN\", user='nasdag', password='', database='default')\n",
    "cur = conn.cursor()\n",
    "#cur.execute(\"drop table heart_disease\")\n",
    "cur.execute(\"CREATE EXTERNAL TABLE heart_disease (\" + sqlStringADD + \"\"\")\n",
    "        COMMENT 'heart disease table'\n",
    "        PARTITIONED BY (crunch_date STRING)\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/user/datacruncher/out/heart_disease'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the raw data, clean it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc.stop()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "st = { 'INT': IntegerType(), 'STRING': StringType() , 'FLOAT': FloatType() , 'BOOLEAN': BooleanType() }\n",
    "\n",
    "fieldsRAW = [StructField(field_name, StringType(), True) for field_name in schemaStringADD.split(',')]\n",
    "schemaRAW = StructType(fieldsRAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=u'63.0', sex=u'1.0', cp=u'1.0', trestbps=u'145.0', chol=u'233.0', fbs=u'1.0', restecg=u'2.0', thalach=u'150.0', exang=u'0.0', oldpeak=u'2.3', slope=u'3.0', ca=u'0.0', thal=u'6.0', diagnosis=u'0.0'),\n",
       " Row(age=u'67.0', sex=u'1.0', cp=u'4.0', trestbps=u'160.0', chol=u'286.0', fbs=u'0.0', restecg=u'2.0', thalach=u'108.0', exang=u'1.0', oldpeak=u'1.5', slope=u'2.0', ca=u'3.0', thal=u'3.0', diagnosis=u'2.0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = sc.textFile('/user/datacruncher/in/processed.*.data').map(lambda l: l.split(','))\n",
    "df = parts.map(lambda p: [None if x=='?' else float(x) for x in p]).toDF(schemaRAW)\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.udf.register(\"t_sex\", lambda x: 'male' if x=='1.0' else 'female' if x=='0.0' else None)\n",
    "sqlContext.udf.register(\"t_cp\", lambda x: 'typ_angina' if x=='1.0' else 'asympt' if x=='4.0' else 'non_angina' if x=='3.0' else 'atyp_angina' if x=='2.0' else None)\n",
    "sqlContext.udf.register(\"t_restecg\", lambda x: 'left_vent_hyper' if x=='2.0' else 'normal' if x=='0.0' else 'st_t_wave_abnormality' if x=='1.0' else None)\n",
    "sqlContext.udf.register(\"t_slope\", lambda x: 'up' if x=='1.0' else 'flat' if x=='2.0' else 'down' if x=='3.0' else None)\n",
    "sqlContext.udf.register(\"t_thal\", lambda x: 'fixed_defect' if x=='6.0' else 'normal' if x=='3.0' else 'reversable_defect' if x=='7.0' else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cast(age as INT) as age, t_sex(sex) as sex, t_cp(cp) as cp, cast(trestbps as INT) as trestbps, cast(chol as INT) as chol, cast(cast(fbs as INT) as BOOLEAN) as fbs, t_restecg(restecg) as restecg, cast(thalach as INT) as thalach, cast(cast(exang as INT) as BOOLEAN) as exang, cast(oldpeak as FLOAT) as oldpeak, t_slope(slope) as slope, cast(ca as INT) as ca, t_thal(thal) as thal, cast(diagnosis as INT) as diagnosis\n"
     ]
    }
   ],
   "source": [
    "sqlStringADD = ', '.join([x1 if x2=='STRING' else 'cast('+x1+' as '+x2+') as '+x1 \\\n",
    "                          for x1, x2 in zip(schemaStringADD.split(','), typeStringADD.split(','))])\n",
    "\n",
    "sqlStringADD = sqlStringADD.replace(' sex,', ' t_sex(sex) as sex,')\n",
    "sqlStringADD = sqlStringADD.replace(' cp,', ' t_cp(cp) as cp,')\n",
    "sqlStringADD = sqlStringADD.replace(' restecg,', ' t_restecg(restecg) as restecg,')\n",
    "sqlStringADD = sqlStringADD.replace(' slope,', ' t_slope(slope) as slope,')\n",
    "sqlStringADD = sqlStringADD.replace(' thal,', ' t_thal(thal) as thal,')\n",
    "sqlStringADD = sqlStringADD.replace(' cast(fbs as BOOLEAN) as fbs,', ' cast(cast(fbs as INT) as BOOLEAN) as fbs,')\n",
    "sqlStringADD = sqlStringADD.replace(' cast(exang as BOOLEAN) as exang,', \n",
    "                                    ' cast(cast(exang as INT) as BOOLEAN) as exang,')\n",
    "print sqlStringADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=63, sex=u'male', cp=u'typ_angina', trestbps=145, chol=233, fbs=True, restecg=u'left_vent_hyper', thalach=150, exang=False, oldpeak=2.299999952316284, slope=u'down', ca=0, thal=u'fixed_defect', diagnosis=0, crunch_date=u'2015-12-18')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.registerTempTable(\"df\")\n",
    "dfWithCrunchDate = sqlContext.sql(\"SELECT \"+sqlStringADD+\", '2015-12-18' as crunch_date FROM df\")\n",
    "dfWithCrunchDate.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append the result to the parquet table and update Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfWithCrunchDate.write.format(\"parquet\").mode('Append').partitionBy('crunch_date').parquet('/user/datacruncher/out/heart_disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"MSCK REPAIR TABLE heart_disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and move the original file to a raw directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -mv /user/datacruncher/in/processed.*.data /user/datacruncher/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=38, sex=u'female', cp=u'asympt', trestbps=110, chol=0, fbs=False, restecg=u'normal', thalach=156, exang=False, oldpeak=0.0, slope=u'flat', ca=None, thal=u'normal', diagnosis=1, crunch_date=u'2015-12-18')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.load(\"/user/datacruncher/out/heart_disease\").repartition(6)\n",
    "df.registerTempTable(\"training_data\")\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try first a logistic regression and keep only 2 classes: sick (1) or healthy (0). So let's select the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=67, sex=u'male', cp=u'asympt', trestbps=160, chol=286, fbs=False, restecg=u'left_vent_hyper', thalach=108, exang=True, oldpeak=1.5, slope=u'flat', ca=3, thal=u'normal', label=1.0),\n",
       " Row(age=57, sex=u'female', cp=u'asympt', trestbps=120, chol=354, fbs=False, restecg=u'normal', thalach=163, exang=True, oldpeak=0.6000000238418579, slope=u'up', ca=0, thal=u'normal', label=0.0),\n",
       " Row(age=44, sex=u'male', cp=u'atyp_angina', trestbps=120, chol=263, fbs=False, restecg=u'normal', thalach=173, exang=False, oldpeak=0.0, slope=u'up', ca=0, thal=u'reversable_defect', label=0.0)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cast(cast(diagnosis as boolean) as double) ##only 2 classes\n",
    "#cast(diagnosis as double)\n",
    "data = sqlContext.sql(\"\"\"SELECT age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, \n",
    "                         slope, ca, thal, cast(cast(diagnosis as boolean) as double) as label \n",
    "                         FROM training_data\"\"\").na.drop()\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and convert string based categorical features to numerical categorical features, then numerical based categorical features (in one column) to numerical continuous features (one column per category) - and assemble a vector from a variety of columns and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#age,sex,   cp,    trestbps,chol,fbs,    restecg,thalach,exang,  oldpeak,slope, ca, thal,  diagnosis\n",
    "#INT,STRING,STRING,INT,     INT, BOOLEAN,STRING, INT,    BOOLEAN,FLOAT,  STRING,INT,STRING,INT\n",
    "\n",
    "sexIndexer = StringIndexer(inputCol=\"sex\", outputCol=\"sexIndex\")\n",
    "sexEncoder = OneHotEncoder(inputCol=\"sexIndex\", outputCol=\"sexVec\")\n",
    "cpIndexer = StringIndexer(inputCol=\"cp\", outputCol=\"cpIndex\")\n",
    "cpEncoder = OneHotEncoder(inputCol=\"cpIndex\", outputCol=\"cpVec\")\n",
    "fbsIndexer = StringIndexer(inputCol=\"fbs\", outputCol=\"fbsIndex\")\n",
    "fbsEncoder = OneHotEncoder(inputCol=\"fbsIndex\", outputCol=\"fbsVec\")\n",
    "restecgIndexer = StringIndexer(inputCol=\"restecg\", outputCol=\"restecgIndex\")\n",
    "restecgEncoder = OneHotEncoder(inputCol=\"restecgIndex\", outputCol=\"restecgVec\")\n",
    "exangIndexer = StringIndexer(inputCol=\"exang\", outputCol=\"exangIndex\")\n",
    "exangEncoder = OneHotEncoder(inputCol=\"exangIndex\", outputCol=\"exangVec\")\n",
    "slopeIndexer = StringIndexer(inputCol=\"slope\", outputCol=\"slopeIndex\")\n",
    "slopeEncoder = OneHotEncoder(inputCol=\"slopeIndex\", outputCol=\"slopeVec\")\n",
    "caIndexer = StringIndexer(inputCol=\"ca\", outputCol=\"caIndex\")\n",
    "caEncoder = OneHotEncoder(inputCol=\"caIndex\", outputCol=\"caVec\")\n",
    "thalIndexer = StringIndexer(inputCol=\"thal\", outputCol=\"thalIndex\")\n",
    "thalEncoder = OneHotEncoder(inputCol=\"thalIndex\", outputCol=\"thalVec\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"age\", \"sexVec\", \"cpVec\", \"trestbps\", \"chol\", \"fbsVec\", \n",
    "                                       \"restecgVec\", \"thalach\", \"exangVec\", \"oldpeak\", \"slopeVec\", \n",
    "                                       \"caVec\", \"thalVec\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]).build()\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1]).addGrid(lr.elasticNetParam, [0.5]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([sexIndexer, sexEncoder, cpIndexer, cpEncoder, fbsIndexer, fbsEncoder, restecgIndexer, restecgEncoder, exangIndexer, exangEncoder, slopeIndexer, slopeEncoder, caIndexer, caEncoder, thalIndexer, thalEncoder, assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name (default: label)\n",
      "metricName: metric name in evaluation (areaUnderROC|areaUnderPR) (default: areaUnderROC)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "rawPrediction\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print evaluator.explainParams()\n",
    "print evaluator.getRawPredictionCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvs = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test = data.randomSplit([0.75, 0.25], seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tvs.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=57, sex=u'female', cp=u'asympt', trestbps=120, chol=354, fbs=False, restecg=u'normal', thalach=163, exang=True, oldpeak=0.6000000238418579, slope=u'up', ca=0, thal=u'normal', label=0.0, sexIndex=1.0, sexVec=SparseVector(1, {}), cpIndex=0.0, cpVec=SparseVector(3, {0: 1.0}), fbsIndex=0.0, fbsVec=SparseVector(1, {0: 1.0}), restecgIndex=0.0, restecgVec=SparseVector(2, {0: 1.0}), exangIndex=1.0, exangVec=SparseVector(1, {}), slopeIndex=1.0, slopeVec=SparseVector(2, {1: 1.0}), caIndex=0.0, caVec=SparseVector(3, {0: 1.0}), thalIndex=0.0, thalVec=SparseVector(2, {0: 1.0}), features=SparseVector(20, {0: 57.0, 2: 1.0, 5: 120.0, 6: 354.0, 7: 1.0, 8: 1.0, 10: 163.0, 12: 0.6, 14: 1.0, 15: 1.0, 18: 1.0}), rawPrediction=DenseVector([0.7801, -0.7801]), probability=DenseVector([0.6857, 0.3143]), prediction=0.0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(test).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911764705882\n",
      "0.916386456872\n"
     ]
    }
   ],
   "source": [
    "print evaluator.evaluate(model.transform(test), {evaluator.metricName: \"areaUnderROC\"})\n",
    "print evaluator.evaluate(model.transform(test), {evaluator.metricName: \"areaUnderPR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "holdout = model.transform(test).select(\"prediction\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800735294118\n",
      "0.85486731075\n"
     ]
    }
   ],
   "source": [
    "cm = BinaryClassificationMetrics(holdout.map(lambda x : (x[0], x[1])))\n",
    "print cm.areaUnderROC\n",
    "print cm.areaUnderPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Random Forest\n",
    "Load and parse the data file, converting it to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sqlContext.sql(\"\"\"SELECT age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, \n",
    "                      slope, ca, thal, cast(diagnosis as boolean) as label \n",
    "                      FROM training_data\"\"\").na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index labels, adding metadata to the label column.\n",
    "Fit on whole dataset to include all labels in index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets (30% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test = data.randomSplit([0.75, 0.25], seed = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a RandomForest model. Chain indexers and tree in a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    " \n",
    "pipeline = Pipeline().setStages([labelIndexer, sexIndexer, sexEncoder, cpIndexer, cpEncoder, fbsIndexer, fbsEncoder, restecgIndexer, restecgEncoder, exangIndexer, exangEncoder, slopeIndexer, slopeEncoder, caIndexer, caEncoder, thalIndexer, thalEncoder, assembler, dt])\n",
    "\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions. Select example rows to display. Select (prediction, true label) and compute test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|prediction|indexedLabel|\n",
      "+----------+------------+\n",
      "|       0.0|         0.0|\n",
      "|       0.0|         0.0|\n",
      "|       0.0|         0.0|\n",
      "|       0.0|         0.0|\n",
      "|       0.0|         0.0|\n",
      "|       1.0|         1.0|\n",
      "|       1.0|         1.0|\n",
      "|       0.0|         0.0|\n",
      "|       0.0|         0.0|\n",
      "|       0.0|         0.0|\n",
      "|       1.0|         1.0|\n",
      "|       0.0|         0.0|\n",
      "|       1.0|         1.0|\n",
      "|       0.0|         1.0|\n",
      "|       1.0|         1.0|\n",
      "+----------+------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "Test Error = 0.202703\n",
      "RandomForestClassificationModel with 20 trees\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"prediction\", \"indexedLabel\").show(15)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print \"Test Error = %g\" % (1.0 - accuracy)\n",
    "\n",
    "treeModel = model.stages[-1]\n",
    "print treeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labelCol: label column name (default: label, current: indexedLabel)\\nmetricName: metric name in evaluation (f1|precision|recall|weightedPrecision|weightedRecall) (default: f1, current: precision)\\npredictionCol: prediction column name (default: prediction, current: prediction)'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894852941176\n",
      "0.89484945751\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print evaluator.evaluate(model.transform(test), {evaluator.metricName: \"areaUnderROC\", evaluator.labelCol: \"indexedLabel\"})\n",
    "print evaluator.evaluate(model.transform(test), {evaluator.metricName: \"areaUnderPR\", evaluator.labelCol: \"indexedLabel\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788235294118\n",
      "0.838485544368\n"
     ]
    }
   ],
   "source": [
    "cm = BinaryClassificationMetrics(model.transform(test).select(\"prediction\",\"indexedLabel\").map(lambda x : (x[0], x[1])))\n",
    "print cm.areaUnderROC\n",
    "print cm.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
