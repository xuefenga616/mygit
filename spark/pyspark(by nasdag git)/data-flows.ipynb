{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to simulate the arrival of ADD, FULL and DELTA flows to our Data Lake. Let's use for this purpose the dump of a retail database. We will load it into mysql and generate csv files to illustrate the arrival of regular flows into our Transfer Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using a password on the command line interface can be insecure.\r\n"
     ]
    }
   ],
   "source": [
    "#! mysql -u hiveuser -phivepassword -D retail_db -e \"DROP DATABASE retail_db\" > /dev/null\n",
    "\n",
    "import pymysql\n",
    "\n",
    "db = pymysql.connect(host=\"localhost\", user='hiveuser', password='hivepassword')\n",
    "cur = db.cursor()\n",
    "cur.execute(\"CREATE DATABASE retail_db\")\n",
    "cur.execute(\"USE retail_db\")\n",
    "cur.execute(open(\"dump.sql\").read())\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tables_in_retail_db\n",
      "0          categories\n",
      "1           customers\n",
      "2         departments\n",
      "3         order_items\n",
      "4              orders\n",
      "5            products\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db = pymysql.connect(host=\"localhost\", user='hiveuser', password='hivepassword', db=\"retail_db\")\n",
    "print pd.read_sql(\"SHOW TABLES\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Field          Type Null  Key Default           Extra\n",
      "0        customer_id       int(11)   NO  PRI    None  auto_increment\n",
      "1     customer_fname   varchar(45)   NO         None                \n",
      "2     customer_lname   varchar(45)   NO         None                \n",
      "3     customer_email   varchar(45)   NO         None                \n",
      "4  customer_password   varchar(45)   NO         None                \n",
      "5    customer_street  varchar(255)   NO         None                \n",
      "6      customer_city   varchar(45)   NO         None                \n",
      "7     customer_state   varchar(45)   NO         None                \n",
      "8   customer_zipcode   varchar(45)   NO         None                \n"
     ]
    }
   ],
   "source": [
    "print pd.read_sql(\"DESCRIBE customers\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id customer_fname customer_lname customer_email customer_password  \\\n",
      "0            1        Richard      Hernandez      XXXXXXXXX         XXXXXXXXX   \n",
      "1            2           Mary        Barrett      XXXXXXXXX         XXXXXXXXX   \n",
      "2            3            Ann          Smith      XXXXXXXXX         XXXXXXXXX   \n",
      "3            4           Mary          Jones      XXXXXXXXX         XXXXXXXXX   \n",
      "4            5         Robert         Hudson      XXXXXXXXX         XXXXXXXXX   \n",
      "\n",
      "           customer_street customer_city customer_state customer_zipcode  \n",
      "0       6303 Heather Plaza   Brownsville             TX            78521  \n",
      "1  9526 Noble Embers Ridge     Littleton             CO            80126  \n",
      "2   3422 Blue Pioneer Bend        Caguas             PR            00725  \n",
      "3       8324 Little Common    San Marcos             CA            92069  \n",
      "4   10 Crystal River Mall         Caguas             PR            00725  \n"
     ]
    }
   ],
   "source": [
    "print pd.read_sql(\"SELECT * FROM customers LIMIT 5\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### generating ADD Flow: daily orders\n",
    "\n",
    "#### Schemas will be stored in memory for this tutorial - we will explore in a later post how to select the right repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id,order_date,order_customer_id,order_status\n"
     ]
    }
   ],
   "source": [
    "pd.read_sql(\"SELECT * FROM orders WHERE order_date='2014-01-01'\", db).to_csv('orders20140101.csv', header=False, index=False, quoting=1)\n",
    "pd.read_sql(\"SELECT * FROM orders WHERE order_date='2014-01-02'\", db).to_csv('orders20140102.csv', header=False, index=False, quoting=1)\n",
    "pd.read_sql(\"SELECT * FROM orders WHERE order_date='2014-01-03'\", db).to_csv('orders20140103.csv', header=False, index=False, quoting=1)\n",
    "\n",
    "schemaStringADD = ','.join(pd.read_sql(\"DESCRIBE orders\", db).Field)\n",
    "print schemaStringADD\n",
    "typeStringADD = 'INT,STRING,INT,STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating FULL Flow: products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id,product_category_id,product_name,product_description,product_price,product_image\n"
     ]
    }
   ],
   "source": [
    "pd.read_sql(\"SELECT * FROM products\", db).to_csv('products20150101.csv', header=False, index=False, quoting=1)\n",
    "pd.read_sql(\"SELECT product_id,product_category_id,product_name,product_description,product_price*10.0,product_image  FROM products\", db).to_csv('products20150102.csv', header=False, index=False, quoting=1)\n",
    "\n",
    "schemaStringFULL = ','.join(pd.read_sql(\"DESCRIBE products\", db).Field)\n",
    "print schemaStringFULL\n",
    "typeStringFULL = 'INT,INT,STRING,STRING,FLOAT,STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating DELTA Flow: customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id,customer_fname,customer_lname,customer_email,customer_password,customer_street,customer_city,customer_state,customer_zipcode\n"
     ]
    }
   ],
   "source": [
    "# zip code of NY based customers became 99999\n",
    "pd.read_sql(\"SELECT customer_id,customer_fname,customer_lname,customer_email,customer_password,customer_street,customer_city,customer_state,'99999' FROM customers WHERE customer_state='NY'\", db).to_csv('customers20150101.csv', header=False, index=False, quoting=1)\n",
    "# zip code of even customer_id is became 00000\n",
    "pd.read_sql(\"SELECT customer_id,customer_fname,customer_lname,customer_email,customer_password,customer_street,customer_city,customer_state,'00000' FROM customers WHERE (customer_id%2)=0\", db).to_csv('customers20150102.csv', header=False, index=False, quoting=1)\n",
    "\n",
    "schemaStringDELTA = ','.join(pd.read_sql(\"DESCRIBE customers\", db).Field)\n",
    "print schemaStringDELTA\n",
    "typeStringDELTA = 'INT,STRING,STRING,STRING,STRING,STRING,STRING,STRING,STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's move the generated files into HDFS\n",
    "\n",
    "#### we will explore later how to move files appropriately between Local and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/21 09:11:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/datacruncher/in\n",
      "15/12/21 09:11:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/datacruncher/raw\n",
      "15/12/21 09:11:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/datacruncher/out\n"
     ]
    }
   ],
   "source": [
    "#! hdfs dfs -rm -r /user/datacruncher/in\n",
    "#! hdfs dfs -rm -r /user/datacruncher/raw\n",
    "#! hdfs dfs -rm -r /user/datacruncher/out\n",
    "! hdfs dfs -mkdir -p /user/datacruncher/in\n",
    "! hdfs dfs -mkdir -p /user/datacruncher/raw\n",
    "! hdfs dfs -put orders*.csv /user/datacruncher/in/\n",
    "! hdfs dfs -put products*.csv /user/datacruncher/in/\n",
    "! hdfs dfs -put customers*.csv /user/datacruncher/in/\n",
    "! rm orders*.csv products*.csv customers*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hive Tables\n",
    "\n",
    "#### let's generate the SQL string from the flow schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id INT, order_date STRING, order_customer_id INT, order_status STRING\n",
      "product_id INT, product_category_id INT, product_name STRING, product_description STRING, product_price FLOAT, product_image STRING\n",
      "customer_id INT, customer_fname STRING, customer_lname STRING, customer_email STRING, customer_password STRING, customer_street STRING, customer_city STRING, customer_state STRING, customer_zipcode STRING\n"
     ]
    }
   ],
   "source": [
    "sqlStringADD = ', '.join([' '.join(x) for x in zip(schemaStringADD.split(','), typeStringADD.split(','))])\n",
    "print sqlStringADD\n",
    "sqlStringFULL = ', '.join([' '.join(x) for x in zip(schemaStringFULL.split(','), typeStringFULL.split(','))])\n",
    "print sqlStringFULL\n",
    "sqlStringDELTA = ', '.join([' '.join(x) for x in zip(schemaStringDELTA.split(','), typeStringDELTA.split(','))])\n",
    "print sqlStringDELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and use JDBC to connect to Hive and create the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyhs2\n",
    "conn = pyhs2.connect(host='localhost', port=10000, authMechanism=\"PLAIN\", user='nasdag', password='', database='default')\n",
    "cur = conn.cursor()\n",
    "#cur.execute(\"drop table orders\")\n",
    "#cur.execute(\"drop table products\")\n",
    "#cur.execute(\"drop table customers\")\n",
    "#cur.execute(\"drop table customers_ref\")\n",
    "cur.execute(\"CREATE EXTERNAL TABLE orders (\" + sqlStringADD + \"\"\")\n",
    "        COMMENT 'orders table'\n",
    "        PARTITIONED BY (crunch_date STRING)\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/user/datacruncher/out/orders'\"\"\")\n",
    "cur.execute(\"CREATE EXTERNAL TABLE products (\" + sqlStringFULL + \"\"\")\n",
    "        COMMENT 'products table'\n",
    "        PARTITIONED BY (crunch_date STRING)\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/user/datacruncher/out/products'\"\"\")\n",
    "cur.execute(\"CREATE EXTERNAL TABLE customers (\" + sqlStringDELTA + \"\"\")\n",
    "        COMMENT 'customers table'\n",
    "        PARTITIONED BY (crunch_date STRING)\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/user/datacruncher/out/customers'\"\"\")\n",
    "cur.execute(\"CREATE EXTERNAL TABLE customers_ref (\" + sqlStringDELTA + \"\"\")\n",
    "        COMMENT 'customers ref table'\n",
    "        PARTITIONED BY (crunch_date STRING)\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/user/datacruncher/out/customers_ref'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start pyspark to process the data flows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc.stop()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to generate also the the appropriate Spark schemas from the schema strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(order_id,IntegerType,false),StructField(order_date,StringType,false),StructField(order_customer_id,IntegerType,false),StructField(order_status,StringType,false)))\n",
      "StructType(List(StructField(product_id,IntegerType,false),StructField(product_category_id,IntegerType,false),StructField(product_name,StringType,false),StructField(product_description,StringType,false),StructField(product_price,FloatType,false),StructField(product_image,StringType,false)))\n",
      "StructType(List(StructField(customer_id,IntegerType,false),StructField(customer_fname,StringType,false),StructField(customer_lname,StringType,false),StructField(customer_email,StringType,false),StructField(customer_password,StringType,false),StructField(customer_street,StringType,false),StructField(customer_city,StringType,false),StructField(customer_state,StringType,false),StructField(customer_zipcode,StringType,false)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "st = { 'INT': IntegerType(), 'STRING': StringType() , 'FLOAT': FloatType() }\n",
    "\n",
    "fieldsADD = [StructField(field_name, st[field_type], False) for field_name, field_type in \\\n",
    "          zip(schemaStringADD.split(','), typeStringADD.split(','))]\n",
    "schemaADD = StructType(fieldsADD)\n",
    "print schemaADD\n",
    "\n",
    "fieldsFULL = [StructField(field_name, st[field_type], False) for field_name, field_type in \\\n",
    "          zip(schemaStringFULL.split(','), typeStringFULL.split(','))]\n",
    "schemaFULL = StructType(fieldsFULL)\n",
    "print schemaFULL\n",
    "\n",
    "fieldsDELTA = [StructField(field_name, st[field_type], False) for field_name, field_type in \\\n",
    "          zip(schemaStringDELTA.split(','), typeStringDELTA.split(','))]\n",
    "schemaDELTA = StructType(fieldsDELTA)\n",
    "print schemaDELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crunching of Flow ADD\n",
    "\n",
    "#### load the csv flow according to its schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schemaADD).load('/user/datacruncher/in/orders20140101.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(order_id=25876, order_date=u'2014-01-01 00:00:00', order_customer_id=3414, order_status=u'PENDING_PAYMENT')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add the crunch date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(order_id=25876, order_date=u'2014-01-01 00:00:00', order_customer_id=3414, order_status=u'PENDING_PAYMENT', crunch_date=u'2015-12-18')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.registerDataFrameAsTable(df, \"df\")\n",
    "dfWithCrunchDate = sqlContext.sql(\"SELECT *, '2015-12-18' as crunch_date FROM df\")\n",
    "dfWithCrunchDate.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append to the parquet table and update Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[result: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithCrunchDate.write.format(\"parquet\").mode('Append').partitionBy('crunch_date').parquet('/user/datacruncher/out/orders')\n",
    "sqlContext.sql(\"MSCK REPAIR TABLE orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### move the original file to a raw directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -mv /user/datacruncher/in/orders20140101.csv /user/datacruncher/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query the content of Hive through JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'comment': '', 'columnName': 'order_id', 'type': 'INT_TYPE'}, {'comment': '', 'columnName': 'order_date', 'type': 'STRING_TYPE'}, {'comment': '', 'columnName': 'order_customer_id', 'type': 'INT_TYPE'}, {'comment': '', 'columnName': 'order_status', 'type': 'STRING_TYPE'}, {'comment': '', 'columnName': 'crunch_date', 'type': 'STRING_TYPE'}]\n",
      "       0                    1      2                3           4\n",
      "0  25876  2014-01-01 00:00:00   3414  PENDING_PAYMENT  2015-12-18\n",
      "1  25877  2014-01-01 00:00:00   5549  PENDING_PAYMENT  2015-12-18\n",
      "2  25878  2014-01-01 00:00:00   9084          PENDING  2015-12-18\n",
      "3  25879  2014-01-01 00:00:00   5118          PENDING  2015-12-18\n",
      "4  25880  2014-01-01 00:00:00  10146         CANCELED  2015-12-18\n",
      "5  25881  2014-01-01 00:00:00   3205  PENDING_PAYMENT  2015-12-18\n",
      "6  25882  2014-01-01 00:00:00   4598         COMPLETE  2015-12-18\n",
      "7  25883  2014-01-01 00:00:00  11764          PENDING  2015-12-18\n",
      "8  25884  2014-01-01 00:00:00   7904  PENDING_PAYMENT  2015-12-18\n",
      "9  25885  2014-01-01 00:00:00   7253          PENDING  2015-12-18\n"
     ]
    }
   ],
   "source": [
    "conn = pyhs2.connect(host='localhost', port=10000, authMechanism=\"PLAIN\", user='nasdag', password='', database='default')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"select * from orders order by order_id limit 10\")\n",
    "print cur.getSchema()\n",
    "print pd.DataFrame(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat the same for FULL: load the incoming data, append it to the parquet table with the crunch date but also create a view on the last set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1                                              2 3            4  \\\n",
      "0  1  2  Quest Q64 10 FT. x 10 FT. Slant Leg Instant U      59.980000   \n",
      "1  2  2  Under Armour Men's Highlight MC Football Clea     129.990005   \n",
      "2  3  2  Under Armour Men's Renegade D Mid Football Cl      89.989998   \n",
      "3  4  2  Under Armour Men's Renegade D Mid Football Cl      89.989998   \n",
      "4  1  2  Quest Q64 10 FT. x 10 FT. Slant Leg Instant U     599.799988   \n",
      "5  2  2  Under Armour Men's Highlight MC Football Clea    1299.900024   \n",
      "6  3  2  Under Armour Men's Renegade D Mid Football Cl     899.899963   \n",
      "7  4  2  Under Armour Men's Renegade D Mid Football Cl     899.899963   \n",
      "\n",
      "                                                   5           6  \n",
      "0  http://images.acmesports.sports/Quest+Q64+10+F...  2015-12-18  \n",
      "1  http://images.acmesports.sports/Under+Armour+M...  2015-12-18  \n",
      "2  http://images.acmesports.sports/Under+Armour+M...  2015-12-18  \n",
      "3  http://images.acmesports.sports/Under+Armour+M...  2015-12-18  \n",
      "4  http://images.acmesports.sports/Quest+Q64+10+F...  2015-12-19  \n",
      "5  http://images.acmesports.sports/Under+Armour+M...  2015-12-19  \n",
      "6  http://images.acmesports.sports/Under+Armour+M...  2015-12-19  \n",
      "7  http://images.acmesports.sports/Under+Armour+M...  2015-12-19  \n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schemaFULL).load('/user/datacruncher/in/products20150101.csv')\n",
    "sqlContext.registerDataFrameAsTable(df, \"df\")\n",
    "dfWithCrunchDate = sqlContext.sql(\"SELECT *, '2015-12-18' as crunch_date FROM df\")\n",
    "dfWithCrunchDate.write.format(\"parquet\").mode('Append').partitionBy('crunch_date').parquet('/user/datacruncher/out/products')\n",
    "sqlContext.sql(\"MSCK REPAIR TABLE products\")\n",
    "! hdfs dfs -mv /user/datacruncher/in/products20150101.csv /user/datacruncher/raw/\n",
    "\n",
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schemaFULL).load('/user/datacruncher/in/products20150102.csv')\n",
    "sqlContext.registerDataFrameAsTable(df, \"df\")\n",
    "dfWithCrunchDate = sqlContext.sql(\"SELECT *, '2015-12-19' as crunch_date FROM df\")\n",
    "dfWithCrunchDate.write.format(\"parquet\").mode('Append').partitionBy('crunch_date').parquet('/user/datacruncher/out/products')\n",
    "sqlContext.sql(\"MSCK REPAIR TABLE products\")\n",
    "! hdfs dfs -mv /user/datacruncher/in/products20150102.csv /user/datacruncher/raw/\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"select * from products where product_id < 5 limit 10\")\n",
    "print pd.DataFrame(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a view for Hive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1                                              2 3            4  \\\n",
      "0  1  2  Quest Q64 10 FT. x 10 FT. Slant Leg Instant U     599.799988   \n",
      "1  2  2  Under Armour Men's Highlight MC Football Clea    1299.900024   \n",
      "2  3  2  Under Armour Men's Renegade D Mid Football Cl     899.899963   \n",
      "3  4  2  Under Armour Men's Renegade D Mid Football Cl     899.899963   \n",
      "\n",
      "                                                   5  \n",
      "0  http://images.acmesports.sports/Quest+Q64+10+F...  \n",
      "1  http://images.acmesports.sports/Under+Armour+M...  \n",
      "2  http://images.acmesports.sports/Under+Armour+M...  \n",
      "3  http://images.acmesports.sports/Under+Armour+M...  \n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE VIEW IF NOT EXISTS prod (\" + schemaStringFULL + \"\"\")\n",
    "            AS SELECT \"\"\" + schemaStringFULL + \"\"\" FROM products\n",
    "            WHERE crunch_date = '2015-12-19' \"\"\")\n",
    "cur.execute(\"select * from prod where product_id < 5 limit 10\")\n",
    "print pd.DataFrame(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### and now for DELTA: load the incoming data, append it to the parquet table with the crunch date but also  - create a consolidated parquet table as reference of all changes and update hive to see the historical and reference tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(customer_id=41, customer_fname=u'Victoria', customer_lname=u'Mason', customer_email=u'XXXXXXXXX', customer_password=u'XXXXXXXXX', customer_street=u'7869 Crystal View Villas', customer_city=u'Brooklyn', customer_state=u'NY', customer_zipcode=u'99999', crunch_date=u'2015-12-18')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schemaDELTA).load('/user/datacruncher/in/customers20150101.csv')\n",
    "sqlContext.registerDataFrameAsTable(df, \"df\")\n",
    "dfWithCrunchDate = sqlContext.sql(\"SELECT *, '2015-12-18' as crunch_date FROM df\")\n",
    "dfWithCrunchDate.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rowNumber, desc\n",
    "dfWithCrunchDate.cache()\n",
    "try:\n",
    "    unionDf = sqlContext.read.parquet('/user/datacruncher/out/customers_ref').unionAll(dfWithCrunchDate)\n",
    "except:\n",
    "    unionDf = dfWithCrunchDate\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\").orderBy(desc(\"crunch_date\"))\n",
    "\n",
    "mergeDf = unionDf.select(\"*\", rowNumber().over(window).alias(\"rowNumber\")).filter(\"rowNumber = 1\").drop(\"rowNumber\")\n",
    "mergeDf.write.format(\"parquet\").mode('Overwrite').partitionBy('crunch_date').parquet('/user/datacruncher/out/customers_merge')\n",
    "sqlContext.read.parquet('/user/datacruncher/out/customers_merge').write.format(\"parquet\").mode('Overwrite').partitionBy('crunch_date').parquet('/user/datacruncher/out/customers_ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[result: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"MSCK REPAIR TABLE customers_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfWithCrunchDate.write.format(\"parquet\").mode('Append').partitionBy('crunch_date').parquet('/user/datacruncher/out/customers')\n",
    "sqlContext.sql(\"MSCK REPAIR TABLE customers\")\n",
    "! hdfs dfs -mv /user/datacruncher/in/customers20150101.csv /user/datacruncher/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second pass with a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schemaDELTA).load('/user/datacruncher/in/customers20150102.csv')\n",
    "sqlContext.registerDataFrameAsTable(df, \"df\")\n",
    "dfWithCrunchDate = sqlContext.sql(\"SELECT *, '2015-12-19' as crunch_date FROM df\")\n",
    "dfWithCrunchDate.cache()\n",
    "try:\n",
    "    unionDf = sqlContext.read.parquet('/user/datacruncher/out/customers_ref').unionAll(dfWithCrunchDate)\n",
    "except:\n",
    "    unionDf = dfWithCrunchDate\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\").orderBy(desc(\"crunch_date\"))\n",
    "\n",
    "mergeDf = unionDf.select(\"*\", rowNumber().over(window).alias(\"rowNumber\")).filter(\"rowNumber = 1\").drop(\"rowNumber\")\n",
    "mergeDf.write.format(\"parquet\").mode('Overwrite').partitionBy('crunch_date').parquet('/user/datacruncher/out/customers_merge')\n",
    "sqlContext.read.parquet('/user/datacruncher/out/customers_merge').write.format(\"parquet\").mode('Overwrite').partitionBy('crunch_date').parquet('/user/datacruncher/out/customers_ref')\n",
    "sqlContext.sql(\"MSCK REPAIR TABLE customers_ref\")\n",
    "\n",
    "dfWithCrunchDate.write.format(\"parquet\").mode('Append').partitionBy('crunch_date').parquet('/user/datacruncher/out/customers')\n",
    "sqlContext.sql(\"MSCK REPAIR TABLE customers\")\n",
    "! hdfs dfs -mv /user/datacruncher/in/customers20150102.csv /user/datacruncher/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1          2          3          4  \\\n",
      "0   41  Victoria      Mason  XXXXXXXXX  XXXXXXXXX   \n",
      "1   46  Jennifer      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "2   46  Jennifer      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "3   60      Mary  Gutierrez  XXXXXXXXX  XXXXXXXXX   \n",
      "4   60      Mary  Gutierrez  XXXXXXXXX  XXXXXXXXX   \n",
      "5   62      Mary      Oneal  XXXXXXXXX  XXXXXXXXX   \n",
      "6   62      Mary      Oneal  XXXXXXXXX  XXXXXXXXX   \n",
      "7   81      Mary      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "8  108    Joshua      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "9  108    Joshua      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "\n",
      "                             5         6   7      8           9  \n",
      "0     7869 Crystal View Villas  Brooklyn  NY  99999  2015-12-18  \n",
      "1         5463 Rocky Autoroute  Freeport  NY  00000  2015-12-19  \n",
      "2         5463 Rocky Autoroute  Freeport  NY  99999  2015-12-18  \n",
      "3            8632 Bright Route   Webster  NY  00000  2015-12-19  \n",
      "4            8632 Bright Route   Webster  NY  99999  2015-12-18  \n",
      "5      2659 Jagged Rabbit View  Brooklyn  NY  99999  2015-12-18  \n",
      "6      2659 Jagged Rabbit View  Brooklyn  NY  00000  2015-12-19  \n",
      "7             8434 Honey Pines    Ithaca  NY  99999  2015-12-18  \n",
      "8  4587 Noble Zephyr Promenade     Bronx  NY  00000  2015-12-19  \n",
      "9  4587 Noble Zephyr Promenade     Bronx  NY  99999  2015-12-18  \n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"select * from customers where customer_state = 'NY' order by customer_id limit 10\")\n",
    "print pd.DataFrame(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1          2          3          4  \\\n",
      "0   41  Victoria      Mason  XXXXXXXXX  XXXXXXXXX   \n",
      "1   46  Jennifer      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "2   60      Mary  Gutierrez  XXXXXXXXX  XXXXXXXXX   \n",
      "3   62      Mary      Oneal  XXXXXXXXX  XXXXXXXXX   \n",
      "4   81      Mary      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "5  108    Joshua      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "6  120     Nancy      Smith  XXXXXXXXX  XXXXXXXXX   \n",
      "7  133      Mary      Lopez  XXXXXXXXX  XXXXXXXXX   \n",
      "8  134     Donna      Gomez  XXXXXXXXX  XXXXXXXXX   \n",
      "9  141      Mary    Mcmahon  XXXXXXXXX  XXXXXXXXX   \n",
      "\n",
      "                             5         6   7      8           9  \n",
      "0     7869 Crystal View Villas  Brooklyn  NY  99999  2015-12-18  \n",
      "1         5463 Rocky Autoroute  Freeport  NY  00000  2015-12-19  \n",
      "2            8632 Bright Route   Webster  NY  00000  2015-12-19  \n",
      "3      2659 Jagged Rabbit View  Brooklyn  NY  00000  2015-12-19  \n",
      "4             8434 Honey Pines    Ithaca  NY  99999  2015-12-18  \n",
      "5  4587 Noble Zephyr Promenade     Bronx  NY  00000  2015-12-19  \n",
      "6        7840 Umber Sky Villas     Bronx  NY  00000  2015-12-19  \n",
      "7       948 Thunder Gate Beach  Brooklyn  NY  99999  2015-12-18  \n",
      "8       1690 Cinder Deer Chase  Brooklyn  NY  00000  2015-12-19  \n",
      "9    1526 Broad Mountain Plaza  Brooklyn  NY  99999  2015-12-18  \n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"select * from customers_ref where customer_state = 'NY' order by customer_id limit 10\")\n",
    "print pd.DataFrame(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
